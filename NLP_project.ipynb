{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PXTbVE0_SEBt"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Imports\n"
      ],
      "metadata": {
        "id": "PXTbVE0_SEBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ufal.udpipe pymorphy2 natasha transformers torch sacremoses"
      ],
      "metadata": {
        "id": "PAKDZMqWq8Qb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import ufal.udpipe\n",
        "import re\n",
        "import pymorphy2\n",
        "from natasha import Segmenter, NewsEmbedding, NewsSyntaxParser, Doc\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5ForConditionalGeneration, T5Tokenizer\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "X2qS6cW0Ar8T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/texts.csv')\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "5wL8FSDN8AUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d696ad4d-525a-4365-f4cf-1ebb036737a2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text\n",
            "0  I've been meaning to write for ages and finall...\n",
            "1  I'm writing this review to give you a heads up...\n",
            "2  Hi Meena, Thank you so much for offering to ho...\n",
            "3  Hi Helga,\\nI've been meaning to write to you f...\n",
            "4  Dear Professor Henley,\\nI am writing to inform...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translators"
      ],
      "metadata": {
        "id": "YfLWZo7Gy759"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##opus-mt-en-ru"
      ],
      "metadata": {
        "id": "zPrRuJxjs8jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_opus = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ru\")\n",
        "model_opus = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-ru\").to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "seljksRxrgQD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_text(text):\n",
        "    input_ids = tokenizer_opus(text, return_tensors=\"pt\").input_ids.to(device)\n",
        "    output_ids = model_opus.generate(input_ids)\n",
        "    translated_text = tokenizer_opus.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return translated_text"
      ],
      "metadata": {
        "id": "Wm8I-gFlrikN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_opus_mt_en_ru = translate_text(df['text'][3])\n",
        "print(text_opus_mt_en_ru)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fvgQPQ1qzcE",
        "outputId": "e4b14690-3023-44bc-f5ff-2e7caabafcf7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Здравствуйте, Хельга, я давно собирался писать вам, так что не волнуйтесь! Как прошли ваши экзамены? Когда вы узнаете результаты? Я уверен, что вы сделали блестяще! Что касается меня, то я был на новой работе три месяца к концу следующей недели, так что я чувствую себя более комфортно. Сначала я чувствовал, что не имею понятия, что я делаю, но теперь я понимаю, что это нормально. Нам нужно было многому научиться - на самом деле, и мне пришлось привыкнуть к мысли, что я не знаю всего. Я привыкла работать допоздна и по выходным, но я медленно вхожу в нормальную рутину. Что означает, что я бы с удовольствием приехала и навестила вас! Нам действительно нужно наверстать упущенное! Я не могу поверить, что мы не видели друг друга после свадьбы Карла. Как звучит в следующем месяце? В любом случае, мне лучше вернуться на работу. Поздравляю вас на новой квартире.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##utrobinmv/t5_translate_en_ru_zh_large_1024"
      ],
      "metadata": {
        "id": "oMAdq2c_sjxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'utrobinmv/t5_translate_en_ru_zh_large_1024'\n",
        "model_utrobinmv = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "model_utrobinmv.to(device)\n",
        "tokenizer_utrobinmv = T5Tokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "12JUuqiFsjEu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = 'translate to ru: '\n",
        "src_text = prefix + df['text'][2]\n",
        "\n",
        "input_ids = tokenizer_utrobinmv(src_text, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "vwB_Rsl5uUhs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_tokens = model_utrobinmv.generate(**input_ids.to(device))\n",
        "\n",
        "text_t5 = tokenizer_utrobinmv.batch_decode(generated_tokens, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "N3SxXe5Tuanj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_t5 = text_t5[0]\n",
        "print(text_t5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0Wcw7tKyqcC",
        "outputId": "880a7ffa-9f9d-43bf-f120-22b9f479da69"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Привет, Миена, спасибо большое за предложение сидеть дома для нас на следующей неделе. Я только сожалею, что мы не сможем догнать должным образом, пока мы не вернемся из нашей поездки. В любом случае, вот все, что я не успею сказать вам в субботу утром. Сигнальный код 7957. Не забудьте установить его, когда вы выходите и помните, чтобы выключить его, как это БУДЕТ! Кроликам нужно кормить один раз в день и ни при каких обстоятельствах они не должны быть разрешены выйти из своей клетки в саду, поскольку соседские кошки напали на них в прошлом. Пожалуйста, можете использовать стиральную машину в дневное время только потому, что она старая и шумная, а соседи жалуются на обратное? Вы не возражаете открыть все окна наверху на час утром, так как дом становится сырым в это время года? Это о нем! Вы найдете все необходимое в доме, и помогите себе во всем в холодильнике или шкафах. Не стесняйтесь позвонить или написать, если у вас есть какие-либо проблемы / вопросы. Увидимся кратко, чтобы передать ключи в субботнее утро и ожидайте надлежащего чата, когда мы вернутся! Еще раз спасибо! Суки\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text formatting"
      ],
      "metadata": {
        "id": "M7XbAFA5zk2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load UDPipe"
      ],
      "metadata": {
        "id": "WAYnw4n6SAx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/russian-syntagrus-ud-2.5-191206.udpipe\"\n",
        "model = ufal.udpipe.Model.load(model_path)"
      ],
      "metadata": {
        "id": "Quy5Gsnx1MAJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = ufal.udpipe.Pipeline(model, \"tokenize\", ufal.udpipe.Pipeline.DEFAULT, ufal.udpipe.Pipeline.DEFAULT, 'matxin')"
      ],
      "metadata": {
        "id": "uZ06RmRf1Qnw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separation of text and preservation of punctuation marks"
      ],
      "metadata": {
        "id": "7OO3IQzC0cUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text = text_t5\n",
        "text = text_opus_mt_en_ru"
      ],
      "metadata": {
        "id": "29oWomZ7zcy6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation_positions = [m.group() for m in re.finditer(r'[\\n«»,.?!:()]+', text)]\n",
        "parts = re.split(r'[«»,.?!:(!)]', text)\n",
        "parts = [part.strip() for part in parts if part.strip()]"
      ],
      "metadata": {
        "id": "FnBEBH-3kvDs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Formatting functions"
      ],
      "metadata": {
        "id": "AhnfqT7KzsmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_dependence(text):\n",
        "  emb = NewsEmbedding()\n",
        "  segmenter = Segmenter()\n",
        "  syntax_parser = NewsSyntaxParser(emb)\n",
        "  doc = Doc(text)\n",
        "  doc.segment(segmenter)\n",
        "  doc.parse_syntax(syntax_parser)\n",
        "\n",
        "  root = ''\n",
        "  dependencies = {}\n",
        "  info_words = {token.id: [token.rel, token.text] for token in doc.tokens}\n",
        "  if len(doc.tokens) > 1:\n",
        "    for token in doc.tokens:\n",
        "      if token.rel == 'root':\n",
        "        root = token.id\n",
        "      dependents = [dep.id for dep in doc.tokens if dep.head_id == token.id]\n",
        "      if dependents:\n",
        "        dependencies[token.id] = dependents\n",
        "    if root == '' and len(dependencies) == 1:\n",
        "      root = next(iter(dependencies.keys()))\n",
        "  else:\n",
        "    token = doc.tokens[0]\n",
        "    root = token.id\n",
        "    dependencies = {token.id: []}\n",
        "  return root, dependencies, info_words"
      ],
      "metadata": {
        "id": "QNgKRqxLTCkO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_node(node, result):\n",
        "    result['1_' + node.attrib.get('ord')] = ['1_' + child.attrib.get('ord') for child in node]\n",
        "    return {key: value for key, value in result.items() if value}"
      ],
      "metadata": {
        "id": "GscSKNcRuUV4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_info(node):\n",
        "  form = node.attrib.get('form')\n",
        "  mi = node.attrib.get('mi')\n",
        "  si = node.attrib.get('si')\n",
        "  properties = mi.split('|')\n",
        "  person = next((prop.split('=')[1] for prop in properties if prop.startswith('Person=')), '')\n",
        "  gender = next((prop.split('=')[1] for prop in properties if prop.startswith('Gender=')), '')\n",
        "  return [form, si, person, gender]"
      ],
      "metadata": {
        "id": "omAdoIYSeMkZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def result_words(text, text_info, root):\n",
        "  if root == '':\n",
        "    parsed_text = pipeline.process(text)\n",
        "    root = ET.fromstring(parsed_text)\n",
        "    res, words_info = {}, {}\n",
        "    for sentence in root.findall('SENTENCE'):\n",
        "      for node in sentence.findall('.//NODE'):\n",
        "        res = parse_node(node, res)\n",
        "\n",
        "    for sentence in root.findall('SENTENCE'):\n",
        "      for node in sentence.findall('.//NODE'):\n",
        "        if '1_' + node.attrib.get('ord') not in words_info:\n",
        "          words_info['1_' + node.attrib.get('ord')] = []\n",
        "        words_info['1_' + node.attrib.get('ord')] = extract_info(node)\n",
        "    words_info = dict(sorted(words_info.items()))\n",
        "\n",
        "    for key in words_info.keys():\n",
        "      if words_info[key][1] == 'root':\n",
        "        root = key\n",
        "\n",
        "    for key in text_info.keys():\n",
        "      text_info[key][0] = words_info[key][1]\n",
        "      text_info[key].extend([words_info[key][-2], words_info[key][-1]])\n",
        "\n",
        "    return root, text_info, res\n",
        "\n",
        "  else:\n",
        "    parsed_text = pipeline.process(text)\n",
        "    root_parse = ET.fromstring(parsed_text)\n",
        "    words_info = {}\n",
        "    for sentence in root_parse.findall('SENTENCE'):\n",
        "      for node in sentence.findall('.//NODE'):\n",
        "        if '1_' + node.attrib.get('ord') not in words_info:\n",
        "          words_info['1_' + node.attrib.get('ord')] = []\n",
        "        words_info['1_' + node.attrib.get('ord')] = extract_info(node)\n",
        "    words_info = dict(sorted(words_info.items()))\n",
        "    for key in text_info.keys():\n",
        "      text_info[key].extend([words_info[key][-2], words_info[key][-1]])\n",
        "    return text_info"
      ],
      "metadata": {
        "id": "-KqcnP9belWS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masc_to_femn(word):\n",
        "  morph = pymorphy2.MorphAnalyzer()\n",
        "  p = morph.parse(word)[0]\n",
        "  fem_word = p.inflect({'femn'})\n",
        "  if fem_word:\n",
        "    fem_word = fem_word.word\n",
        "    if word.istitle():\n",
        "      fem_word = word[0] + fem_word[1:]\n",
        "    return fem_word, 'femn'\n",
        "  else:\n",
        "    return word, 'Masc'"
      ],
      "metadata": {
        "id": "GbBM08TD4YtT"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dfs_forms(tree, root, info_nodes, len_sentence, punct, previous_change):\n",
        "  flag = False\n",
        "  if len_sentence == 1: # если часть предложения из 1 слова\n",
        "    if info_nodes[root][2] in ['', '1'] and info_nodes[root][3] == 'Masc':\n",
        "      info_nodes[root][1], info_nodes[root][-1] = masc_to_femn(info_nodes[root][1])\n",
        "      flag = True\n",
        "  else:\n",
        "    nsubj = ''\n",
        "    for key in tree[root]:\n",
        "      if info_nodes[key][0] in ['nsubj', 'nsubj:pass'] and info_nodes[key][3] != 'Neut' and info_nodes[key][1].lower() != 'мы':\n",
        "        nsubj = key\n",
        "    if nsubj == '':\n",
        "      for key in info_nodes.keys():\n",
        "        if info_nodes[key][0] == 'nsubj' and info_nodes[key][1].lower() == 'я':\n",
        "          nsubj = key\n",
        "    if nsubj != '':\n",
        "      if info_nodes[nsubj][3] != 'Masc' and info_nodes[nsubj][2] != '3': # Проход по зависимым словам от подлежащего\n",
        "        if nsubj in tree.keys():\n",
        "          for key in tree[nsubj]:\n",
        "            if info_nodes[key][0] in ['obj', 'aux:pass', 'cop'] and info_nodes[key][3] in ['', 'Masc']:\n",
        "              info_nodes[key][1], info_nodes[key][3] = masc_to_femn(info_nodes[key][1])\n",
        "              flag = True\n",
        "        if info_nodes[root][2] in ['', '1']: # Проход по зависимым словам от корня\n",
        "          info_nodes[root][1], info_nodes[root][-1] = masc_to_femn(info_nodes[root][1])\n",
        "          flag = True\n",
        "          for key in tree[root]:\n",
        "            if info_nodes[key][0] in ['obj', 'aux:pass', 'cop', 'acl'] and info_nodes[key][3] in ['', 'Masc']:\n",
        "              info_nodes[key][1], info_nodes[key][3] = masc_to_femn(info_nodes[key][1])\n",
        "              flag = True\n",
        "    else: # Если нет подлежащего (подчиненная часть сложного предложения)\n",
        "      if punct == ',' and previous_change:\n",
        "        if info_nodes[root][2] in ['', '1'] and info_nodes[root][3] == 'Masc':\n",
        "          info_nodes[root][1], info_nodes[root][-1] = masc_to_femn(info_nodes[root][1])\n",
        "          flag = True\n",
        "          for key in tree[root]:\n",
        "            if info_nodes[key][0] in ['obj', 'aux:pass'] and info_nodes[key][3] in ['', 'Masc']:\n",
        "              info_nodes[key][1], info_nodes[key][3] = masc_to_femn(info_nodes[key][1])\n",
        "              flag = True\n",
        "  return flag"
      ],
      "metadata": {
        "id": "1T8oQFF4-pKi"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_form(part_sentence, punct, prev_change):\n",
        "  root, depend, info = word_dependence(part_sentence)\n",
        "  if not root:\n",
        "    root, info, depend = result_words(part_sentence, info, root)\n",
        "  else:\n",
        "    info = result_words(part_sentence, info, root)\n",
        "\n",
        "  previous = dfs_forms(depend, root, info, len(part_sentence.split()), punct, prev_change)\n",
        "  stroka = ' '.join(info[key][1] for key in info.keys())\n",
        "\n",
        "  return stroka, previous"
      ],
      "metadata": {
        "id": "8K1lGijgcAjI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Starting the translation formatting"
      ],
      "metadata": {
        "id": "mtY_RaBv2MPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = ''\n",
        "for i in range(len(parts)):\n",
        "  if i == 0:\n",
        "    temp, prev_change = correct_form(parts[i], '', False)\n",
        "  else:\n",
        "    temp, prev_change = correct_form(parts[i], punctuation_positions[i-1], prev_change)\n",
        "  if i != len(parts)-1:\n",
        "    result += temp + punctuation_positions[i]\n",
        "  else:\n",
        "    if len(parts) != len(punctuation_positions):\n",
        "      result += temp\n",
        "    else:\n",
        "      result += temp + punctuation_positions[i]\n",
        "  if i != len(parts) - 1:\n",
        "    result += ' '"
      ],
      "metadata": {
        "id": "PtCGzv6CdsVq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYULcZIkDU6I",
        "outputId": "0e7b7708-4bb8-45bc-cb47-47c2a5dbb102"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Здравствуйте, Хельга, я давно собирался писать вам, так что не волнуйтесь! Как прошли ваши экзамены? Когда вы узнаете результаты? Я уверен, что вы сделали блестяще! Что касается меня, то я был на новой работе три месяца к концу следующей недели, так что я чувствую себя более комфортно. Сначала я чувствовал, что не имею понятия, что я делаю, но теперь я понимаю, что это нормально. Нам нужно было многому научиться - на самом деле, и мне пришлось привыкнуть к мысли, что я не знаю всего. Я привыкла работать допоздна и по выходным, но я медленно вхожу в нормальную рутину. Что означает, что я бы с удовольствием приехала и навестила вас! Нам действительно нужно наверстать упущенное! Я не могу поверить, что мы не видели друг друга после свадьбы Карла. Как звучит в следующем месяце? В любом случае, мне лучше вернуться на работу. Поздравляю вас на новой квартире.\n",
            "Здравствуйте, Хельга, я давно собиралась писать вам, так что не волнуйтесь! Как прошли ваши экзамены? Когда вы узнаете результаты? Я уверенна, что вы сделала блестяще! Что касается меня, то я была на новой работе три месяца к концу следующей недели, так что я чувствовала себя более комфортно. Сначала я чувствовала, что не имела понятия, что я делала, но теперь я понимала, что это нормально. Нам нужно было многому научиться - на самом деле, и мне пришлось привыкнуть к мысли, что я не знала всего. Я привыкла работать допоздна и по выходным, но я медленно входила в нормальную рутину. Что означает, что я бы с удовольствием приехала и навестила вас! Нам действительно нужно наверстать упущенное! Я не могла поверить, что мы не видели друг друга после свадьбы Карла. Как звучит в следующем месяце? В любом случае, мне лучше вернуться на работу. Поздравляю вас на новой квартире.\n"
          ]
        }
      ]
    }
  ]
}